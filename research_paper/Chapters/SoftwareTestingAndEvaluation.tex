\chapter{Software testing and evaluation}\label{chap:nine}
After introducing the software components proposed by this article for testing vehicles in a simulated environment, we will now look at how the tools can be used in practice. Specifically, we will discuss how the software was tested in a lab with five human participants and an AI agent. We start in \autoref{sect-9.1} by discussing how the participants were chosen and what had to be done to facilitate their participation in the simulations. Next, in \autoref{sect-9.2}, we will look at how the simulations took place and talk about the things observed during the process. Finally, in \autoref{sect-9.3}, we conclude the chapter by discussing the results of the simulations, comparing the safety performance scores of the human drivers and an AI agent and evaluating how well the proposed system operates.

\section{Selecting the participants} \label{sect-9.1}
For this project, it was decided that the best way to test the developed software is by inviting external participants to drive in the generated scenarios, observing how monitors collected data, and running data analysis evaluating their performance.

As this project aims to create a framework for evaluating autonomous vehicle safety, some AI implementations had to be involved in the process. Initially, the leading candidate for participation was the Autoware.AI framework allowing a vehicle to be controlled by the Autoware software in the simulated environment. The reason it was such a good candidate and remains one is that Autoware.AI is a sophisticated software bundle, implementing a level four autonomous agent (discussed in \autoref{sect-2.3}), possessing the ability to participate in traffic. It is also relatively easy to connect it with the CARLA simulator by connecting Autoware.AI and CARLA using ROS and Autoware bridges provided by the respective developers.

However, due to the complexity of both software systems, the high learning curve and issues not allowing the Autoware.AI agent to follow a set of waypoints, it was decided to use a more straightforward implementation. As the purpose of this project is not to connect different AI agent implementations with the simulator and run them but rather to create a software system for their safety analysis, the choice of the AI participant in the research holds very little significance. For this reason, CARLA BasicAgent was chosen to participate in the software testing.

While still in the initial steps of the project development, it was already established that for the AVs to be considered safe and finally witness the daylight, they have to fit people's safety requirements. It was then that the idea of comparing human drivers and AI agents in the simulated environment made its way onto a piece of paper. It is intriguing how both AI-controlled vehicles and humans would behave in the same situations. For this reason, from the beginning, it was clear that the project needed to be developed to facilitate user testing in the lab.

The first step was to decide how many participants should take part in the research and what prerequisites are needed to allow this process. It was decided that having five people driving in the scenarios would give a good overview of general trends and make for exciting research. As with all projects involving human research, an ethics committee approval was required. For this, an application was submitted, and a positive response came back, allowing the research to occur. The ethical review reference number is LRU-22/23-34770.

Moving on, it is evident that for a participant to reflect the behaviour of a typical driver in the simulated environment, he needs to have had some experience driving on the roads. For this reason, the only requirement for a person to participate in the research was to have a valid driver's license. The licenses were not inspected, but it was assumed that the participants willingly participating in the research were truthful. Five participants were found, and all agreed to take part. Their consent forms are stored safely together with the project files.

While the AI agent only requires the software to drive in the scenarios, the human participants require controllers to operate cars. In order to be as reflective of the real-world conditions as possible, the research had to provide the participants with the tools they are accustomed to when it comes to driving a car: the steering wheel and pedals. For this reason, it was decided to run a controlled experiment in the university lab containing all the necessary hardware.

In addition to the physical tools, software scripts allowing the vehicles to be controlled by both people and AI were needed. For this, manual driving scripts were borrowed from the example scripts provided by the CARLA development team that can be found in the Carla/PythonAPI/examples directory. The scripts were then modified to suit the purpose of the project. The scrips are named driver\_keyboard.py and driver\_steeringwheel.py for the human participants and the self\_driver.py for the CARLA agent. The scripts can be found in the software/carla\_scripts directory.

\section{Human drivers vs. AI} \label{sect-9.2}

The research took place in a Bush House computer lab (30 Aldwych, London WC2B 4BG, United Kingdom). The lab contained the steering wheel and pedal-equipped powerful computers allowing the simulations to run at the highest quality graphics. This ensured that the impact of the uncontrolled variables (concerning how people perceive the environment) was minimised. The technical specifications of the computer used can be seen in \autoref{specs}.

All the participants were tested in five scenarios generated by the scenario generation and path-building algorithms. As discussed in \autoref{chap:three}, when we examined the road incident statistics, the generated scenarios must be diverse to reflect various road conditions. For this reason, the five scenarios cover both day and night conditions, rainy and sunny weather, urban and rural areas and highways. Some maps had their start and finish coordinates altered slightly to make the vehicle start or finish in places like a sideline or a parking lot space. This is due to the shortcomings of the path-building algorithm discussed in \autoref{chap:five}. More about it will be discussed in the following chapter, where we conclude the project by talking about the issues experienced and possible future improvements.

For the experience to be identical for all the participants, the same vehicle was used throughout all the scenarios' simulations. In addition, randomisation seeds were used with each scenario, guaranteeing identical vehicle behaviour and their arrangement on the map. The scenarios that were driven can be found in the software/carla\_scripts/scenario\_list.json file. The fields ``name" and ``details" could be changed to have values pointing to $null$, thus resulting in entirely new and random scenarios being generated. However, as this research aims to get an accurate overview of how people drive, all the scenarios had to be identical for all the participants. That is why randomisation seeds and predefined scenarios were used.

However, ensuring an equal experience for all participants proved to be a challenging endeavour, fraught with various obstacles. To begin with, technical glitches posed a significant problem, such as the steering wheel powering down during simulations or the accelerator pedal giving impulses as if it was being pushed despite having no input. Issues like these resulted in specific scenarios being rerun. Additionally, unforeseen misbehaviour from certain vehicles within the simulation also had a significant effect, mainly on the scenarios themselves. This includes vehicles flying into the air after a collision and landing somewhere else, causing gridlock on the road and thus impacting the flow of traffic.

As reflected by the participants, although the steering wheel and pedals were there, what was lacking was the feeling of actually sitting in a car and being able to turn the head and observe the environment. This was not well conveyed by the static monitor giving the view of the world only at a specific angle at the time. In addition, the participants said they were missing the noises from the environment as the monitors and the CARLA simulator did not provide any sounds. Some also remarked that the steering wheel was too sensitive, and they did not feel the car well. Having said that, it is safe to say that the fact that the participants were driving the car in a virtual environment had an effect, presumably negative, on their performance.

What was also noted from the simulation runs was that the participants found driving at night more difficult than during bright conditions. Overall, the participants said that they enjoyed testing their driving skills in the simulator and gave positive verbal feedback about the process of the research.

All five participants successfully completed all five predefined scenarios. The data collected from their scenario runs can be found in the directory data/recordings from participant\_A until participant\_E.

Regarding the CARLA agent participating in the simulations, there were no technical issues with the equipment. However, there were some problems with how the vehicle executed the given path. At some points during a run, the vehicle would make a 360 degrees turn and then continue driving the path. Although the exact cause remains undetermined, it most likely was provoked by the inaccuracies caused by the GlobalRoutePlanner, which created the path. The run of the scenarios by the CARLA agent can be found in the participant\_CARLA directory.


\section{The outcome} \label{sect-9.3}

In this section, we will evaluate the performance of the safety monitoring tools considering the data they obtained from the scenario executions. In addition, we will utilise the data analysis tools introduced in \autoref{chap:seven} to help us assess vehicle safety and provide some insights into where the most issues occurred.

Let us begin by analysing whether the monitoring tools had done their duties. As mentioned previously, the flow of each simulation session is captured in log files found in the subdirectories of the data/recordings directory. The log files are associated with each participant and record how the simulations were fulfilled, whether the monitors successfully recorded data to their intended files, etc. After examining each session\_logs.log file, no errors or misbehaviours were spotted, meaning that simulation sessions were completed as planned. After looking at each scenario directory, it was confirmed that all the files were present. Having established that no errors occurred when creating the files and no issues were spotted in the simulation session records, it was time to look at the contents of the recording files to see whether they contained the right data in the correct format.

This process was performed manually because no intelligent software system was developed for checking the correctness of the files.

After observing the files, it was confirmed that the data was recorded following the XML file structure and formatting conventions described in the code. The collision files contained the instances of collisions, where they occurred and at what time; the other files contained their respective violation data. The general accuracy of data was checked by replaying some of the simulations and ensuring that the recorded violations occurred. The majority of the violation data was captured as planned.

However, there were instances that brought inaccurate data, thus affecting the safety score of the participants. While replaying the simulations, it was observed that whenever another car bumped into the participant's car, a collision was recorded, giving the participant penalty points for situations where he was the victim. In addition, there were some instances where a collision with one vehicle was recorded multiple times, giving more penalty points than intended. This can be seen in the collision data of the participant\_D in scenario 2.

The first issue with the collision monitor was caused by the simplified implementation of the CollisionSensor, which only considers the instances when another object entered the vehicle's bounding box. It was not distinguishing between the driver running into another object and an object running into the driver. The second issue of the monitor recording the same data multiple times was confirmed to be temporary wrongdoing by the CollisionSensor, as no other instances like this were present. Overall, it is safe to say that the monitors were performing their tasks, although sometimes not doing them right.

Having examined the work of the safety monitors and established that they mostly recorded the right violations, it is now time to use the data analysis tools to analyse the observations even more and evaluate the performance of each of the participants. For that, the data analysis tool was employed. After specifying all the scenarios, all the metrics and all the participants in the analysis\_parameters.json file, the scenario analyser was run and completed the analysis successfully. The analysis can be found in the data/analysed\_data/data directory under the name example\_analysis. After observing the participants.xml and scenarios.xml file content and manually checking that the values determined by the analyser were correctly calculated, it was confirmed that the analysis tool behaved as expected, producing the correct scores. For example, the sum of all the collision penalty points gathered by the participant\_A over all five scenarios is 5750. The analyser correctly tells us that the collision point average of this participant is 1150, given that he drove in five scenarios. The same calculations were manually performed on other metrics to ensure the correctness of the data analysis tool.

Similarly, the point extraction (from the points subdirectory in the analysis folder) was checked to be correct by looking at the coordinates in each file and ensuring they are present in the recording files. This check was performed on a simpler (one participant) analysis because checking for each point in the example\_analysis/points would take a very long time. Nevertheless, if the algorithm behaves correctly with few participants, it is safe to assume that it will also behave correctly with many participants.

The visualisation tools were tested using various metrics from the analysed data. Although the diagrams comparing the scores were being drawn correctly, the map drawing tool marking the locations on the map was sometimes accurately reflecting the coordinates and sometimes marking the points where there was no road. The correctly represented points can be seen in \autoref{fig:correctly_marked}, and the incorrectly marked points can be seen in  \autoref{fig:incorrectly_marked} in \autoref{chap:a}. Due to the complexity of the CommonRoad Scenario Designer and lack of documentation, it was challenging to determine the root cause of why the points are being represented as they are. For this reason, this issue remains unsolved.

It is now time to move on to the safety evaluation bit. For this, the score\_calculator.py script was used, whose purpose was described in \autoref{chap:seven}. The correctness of the algorithm was verified by running the algorithm to evaluate the performance and then manually recalculating the score and comparing the values. This method proved that the algorithm behaves as expected and accurately applies the formula introduced in \autoref{chap:six}.

However, the algorithm is not using the formula to its fullest because of the lack of information retrievable from the simulation environment and the map description files. For instance, there is no straightforward way of retrieving the allowed speed limit in a specific road element using the CARLA simulator. Instead, they can be retrieved from the road elements specified in the OpenDRIVE map descriptions. However, as explained in \autoref{chap:seven}, most road elements in the map descriptions do not provide any information about the allowed speed limits. Because of this, a lot of assumptions had to be made. For example, if the speed limit on the road element is missing, the maximum allowed speed is assumed to be 50km/h. This can affect the final score dramatically if the permitted speed is 30km/h, but the algorithm assumes that it is 50km/h, thus calculating the lower optimum time value to complete the route resulting in the driver either speeding to make it to the finish line on time or losing the points for driving too slow. Similar issues occurred with counting the mandatory stops on the map, mainly because there is no way of retrieving the number of stop signs or pedestrian crossings on the road.
For this reason, an assumption was made that the vehicle would have to wait an average of 12 seconds at each junction. This value is arbitrary and does not serve as an accurate measure. However, it is impossible to accurately determine such metrics without more data.

Having said all that, the score calculator was used to calculate the scores of all the participants in each scenario. The formula used $\gamma$ value 0.7, meaning that the collected data about the penalty points is 70\% accurate. The highest score achieved was 435.661 by the participant\_C in scenario4. The scenario's perfect score, calculated using the formula, was 500 points and above. The ideal score is achieved by completing the route in full, in the exact time given and without doing any violations. If the driver completes the course even quicker than the calculated optimum time, he gets more points. The lowest score achieved was -18979.65 by participant\_D in scenario2. The score was so negative because of the issues with the collision monitor mentioned before, where one collision was recorded multiple times resulting in many penalty points given to the driver.

Overall, the scores of all the participants were mostly highly negative because of the large amounts of penalty points given for each penalty. This can be addressed by adjusting the penalty points assigned for each violation. Also, it was observed that the participants also performed lots of violations, such as accidentally crossing solid lane markings and not showing turn indicators when turning or headlights in the dark, adding to significant negative scores. Out of all six participants from the research, the CARLA agent ended up being the fourth in terms of all the penalty points obtained, which is 10237.
